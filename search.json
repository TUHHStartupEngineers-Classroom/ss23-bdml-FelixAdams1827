[
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html",
    "href": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "",
    "text": "Your organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Salesforce CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nYou will be using stock prices in this analysis. You come up with a method to classify companies based on how their stocks trade using their daily stock returns (percentage movement from one day to the next). This analysis will help your organization determine which companies are related to each other (competitors and have similar attributes).\nYou can analyze the stock prices using what you’ve learned in the unsupervised learning tools including K-Means and UMAP. You will use a combination of kmeans() to find groups and umap() to visualize similarity of daily stock returns."
  },
  {
    "objectID": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "href": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.1 Step 1 - Convert stock prices to a standardized format (daily returns)",
    "text": "5.1 Step 1 - Convert stock prices to a standardized format (daily returns)\nWhat you first need to do is get the data in a format that can be converted to a “user-item” style matrix. The challenge here is to connect the dots between what we have and what we need to do to format it properly.\nWe know that in order to compare the data, it needs to be standardized or normalized. Why? Because we cannot compare values (stock prices) that are of completely different magnitudes. In order to standardize, we will convert from adjusted stock price (dollar value) to daily returns (percent change from previous day). Here is the formula.\n\\[\nreturn_{daily} = \\frac{price_{i}-price_{i-1}}{price_{i-1}}\n\\]\nFirst, what do we have? We have stock prices for every stock in the SP 500 Index, which is the daily stock prices for over 500 stocks. The data set is over 1.2M observations.\n\nsp_500_prices_tbl %>% glimpse()\n\n#> Rows: 1,225,765\n#> Columns: 8\n#> $ symbol   <chr> \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT…\n#> $ date     <date> 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, …\n#> $ open     <dbl> 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53…\n#> $ high     <dbl> 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68…\n#> $ low      <dbl> 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01…\n#> $ close    <dbl> 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09…\n#> $ volume   <dbl> 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5…\n#> $ adjusted <dbl> 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1…\n\n\nYour first task is to convert to a tibble named sp_500_daily_returns_tbl by performing the following operations:\n\nSelect the symbol, date and adjusted columns\nFilter to dates beginning in the year 2018 and beyond.\nCompute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame.\nRemove a NA values from the lagging operation\nCompute the difference between adjusted and the lag\nCompute the percentage difference by dividing the difference by that lag. Name this column pct_return.\nReturn only the symbol, date, and pct_return columns\nSave as a variable named sp_500_daily_returns_tbl\n\n\n\n# Apply your data transformation skills!\n\n# Output: sp_500_daily_returns_tbl"
  },
  {
    "objectID": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-2---convert-to-user-item-format",
    "href": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-2---convert-to-user-item-format",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.2 Step 2 - Convert to User-Item Format",
    "text": "5.2 Step 2 - Convert to User-Item Format\nThe next step is to convert to a user-item format with the symbol in the first column and every other column the value of the daily returns (pct_return) for every stock at each date.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nsp_500_daily_returns_tbl <- read_rds(\"sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n\nNow that we have the daily returns (percentage change from one day to the next), we can convert to a user-item format. The user in this case is the symbol (company), and the item in this case is the pct_return at each date.\n\nSpread the date column to get the values as percentage returns. Make sure to fill an NA values with zeros.\nSave the result as stock_date_matrix_tbl\n\n\n\n# Convert to User-Item Format\n\n# Output: stock_date_matrix_tbl"
  },
  {
    "objectID": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-3---perform-k-means-clustering",
    "href": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-3---perform-k-means-clustering",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.3 Step 3 - Perform K-Means Clustering",
    "text": "5.3 Step 3 - Perform K-Means Clustering\nNext, we’ll perform K-Means clustering.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nstock_date_matrix_tbl <- read_rds(\"stock_date_matrix_tbl.rds\")\n\nBeginning with the stock_date_matrix_tbl, perform the following operations:\n\nDrop the non-numeric column, symbol\n\nPerform kmeans() with centers = 4 and nstart = 20\n\nSave the result as kmeans_obj\n\n\n\n# Create kmeans_obj for 4 centers\n\nUse glance() to get the tot.withinss.\n\n# Apply glance() to get the tot.withinss"
  },
  {
    "objectID": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-4---find-the-optimal-value-of-k",
    "href": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-4---find-the-optimal-value-of-k",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.4 Step 4 - Find the optimal value of K",
    "text": "5.4 Step 4 - Find the optimal value of K\nNow that we are familiar with the process for calculating kmeans(), let’s use purrr to iterate over many values of “k” using the centers argument.\nWe’ll use this custom function called kmeans_mapper():\n\nkmeans_mapper <- function(center = 3) {\n    stock_date_matrix_tbl %>%\n        select(-symbol) %>%\n        kmeans(centers = center, nstart = 20)\n}\n\nApply the kmeans_mapper() and glance() functions iteratively using purrr.\n\nCreate a tibble containing column called centers that go from 1 to 30\nAdd a column named k_means with the kmeans_mapper() output. Use mutate() to add the column and map() to map centers to the kmeans_mapper() function.\nAdd a column named glance with the glance() output. Use mutate() and map() again to iterate over the column of k_means.\nSave the output as k_means_mapped_tbl\n\n\n\n# Use purrr to map\n\n\n# Output: k_means_mapped_tbl \n\nNext, let’s visualize the “tot.withinss” from the glance output as a Scree Plot.\n\nBegin with the k_means_mapped_tbl\n\nUnnest the glance column\nPlot the centers column (x-axis) versus the tot.withinss column (y-axis) using geom_point() and geom_line()\n\nAdd a title “Scree Plot” and feel free to style it with your favorite theme\n\n\n# Visualize Scree Plot\n\nWe can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K."
  },
  {
    "objectID": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-5---apply-umap",
    "href": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-5---apply-umap",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.5 Step 5 - Apply UMAP",
    "text": "5.5 Step 5 - Apply UMAP\nNext, let’s plot the UMAP 2D visualization to help us investigate cluster assignments.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"k_means_mapped_tbl.rds\")\n\nFirst, let’s apply the umap() function to the stock_date_matrix_tbl, which contains our user-item matrix in tibble format.\n\nStart with stock_date_matrix_tbl\n\nDe-select the symbol column\nUse the umap() function storing the output as umap_results\n\n\n\n# Apply UMAP\n\n# Store results as: umap_results \n\nNext, we want to combine the layout from the umap_results with the symbol column from the stock_date_matrix_tbl.\n\nStart with umap_results$layout\n\nConvert from a matrix data type to a tibble with as_tibble()\n\nBind the columns of the umap tibble with the symbol column from the stock_date_matrix_tbl.\nSave the results as umap_results_tbl.\n\n\n# Convert umap results to tibble with symbols\n\n# Output: umap_results_tbl\n\nFinally, let’s make a quick visualization of the umap_results_tbl.\n\nPipe the umap_results_tbl into ggplot() mapping the columns to x-axis and y-axis\nAdd a geom_point() geometry with an alpha = 0.5\n\nApply theme_tq() and add a title “UMAP Projection”\n\n\n# Visualize UMAP results\n\nWe can now see that we have some clusters. However, we still need to combine the K-Means clusters and the UMAP 2D representation."
  },
  {
    "objectID": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-6---combine-k-means-and-umap",
    "href": "content/01_journal/FUNDAMENTAL_DOWNLOADS/Chapter_1_Challenge.html#step-6---combine-k-means-and-umap",
    "title": "Session 6 - Challenge - Company Segmentation",
    "section": "\n5.6 Step 6 - Combine K-Means and UMAP",
    "text": "5.6 Step 6 - Combine K-Means and UMAP\nNext, we combine the K-Means clusters and the UMAP 2D representation\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"k_means_mapped_tbl.rds\")\numap_results_tbl   <- read_rds(\"umap_results_tbl.rds\")\n\nFirst, pull out the K-Means for 10 Centers. Use this since beyond this value the Scree Plot flattens. Have a look at the business case to recall how that works.\n\n# Get the k_means_obj from the 10th center\n\n# Store as k_means_obj\n\nNext, we’ll combine the clusters from the k_means_obj with the umap_results_tbl.\n\nBegin with the k_means_obj\n\nAugment the k_means_obj with the stock_date_matrix_tbl to get the clusters added to the end of the tibble\nSelect just the symbol and .cluster columns\nLeft join the result with the umap_results_tbl by the symbol column\nLeft join the result with the result of sp_500_index_tbl %>% select(symbol, company, sector) by the symbol column.\nStore the output as umap_kmeans_results_tbl\n\n\n\n# Use your dplyr & broom skills to combine the k_means_obj with the umap_results_tbl\n\n\n# Output: umap_kmeans_results_tbl \n\nPlot the K-Means and UMAP results.\n\nBegin with the umap_kmeans_results_tbl\n\nUse ggplot() mapping V1, V2 and color = .cluster\n\nAdd the geom_point() geometry with alpha = 0.5\n\nApply colors as you desire (e.g. scale_color_manual(values = palette_light() %>% rep(3)))\n\n\n# Visualize the combined K-Means and UMAP results\n\nCongratulations! You are done with the 1st challenge!"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/challenge_1.html",
    "href": "content/01_journal/challenge_1.html",
    "title": "Challenge 1",
    "section": "",
    "text": "1 Loading libraries and data\n\n# Load libraries \n    library(tidyverse)\n    library(tidyquant)\n    library(broom)\n    library(umap)\n    library(readr)\n    library(readxl)\n\n# STOCK PRICES and SECTOR INFORMATION\n\n  sp_500_prices_tbl <- read_rds(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/FUNDAMENTAL_DOWNLOADS/sp_500_prices_tbl.rds\")\n  \n  sp_500_index_tbl <- read_rds(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/FUNDAMENTAL_DOWNLOADS/sp_500_index_tbl.rds\")\n\n\n2 Conversion of the stock prices to a standardized format (daily returns)\n\n  sp_500_daily_returns_tbl <- sp_500_prices_tbl %>% \n  select(symbol, date, adjusted) %>% \n  filter(date >= '2018-01-01') %>% \n  group_by(symbol) %>% \n  mutate(lag = lag(adjusted)) %>% \n  na.omit %>% \n  mutate(pct_return = (adjusted - lag)/lag) %>% \n  select(symbol, date, pct_return)\n\n\n3 Conversion to the User-Item format\n\n  stock_date_matrix_tbl <- sp_500_daily_returns_tbl %>% \n  spread(date, pct_return) %>% \n  replace(is.na(.),0) %>% \n  ungroup()\n\n\n4 Performing K-Means clustering\n\n  kmeans_obj <- stock_date_matrix_tbl %>% \n  within(rm(\"symbol\")) %>% \n  kmeans(centers = 4, nstart = 20)\n\n\n5 Finding the optimal value of K\n\nkmeans_mapper <- function(center = 3) {\n  stock_date_matrix_tbl %>%\n    select(-symbol) %>%\n    kmeans(centers = center, nstart = 20)\n}\n\nk_means_mapped_tbl <- tibble(centers = 1:30) %>% \n  mutate(k_means = centers %>%  map(kmeans_mapper)) %>% \n  mutate(glance = k_means %>% map(glance))\n  \nk_means_mapped_tbl %>% \n  unnest(glance) %>% \n  select(centers, tot.withinss) %>% \n  \n  \n# Visualization of tot.withinss\n  \n  ggplot(aes(centers, tot.withinss)) +\n  geom_point(color = \"#2DC6D6\", size = 5) +\n  geom_line(color = \"#2DC6D6\", linewidth = 2) +\n  # Add labels (which are repelled a little)\n  ggrepel::geom_label_repel(aes(label = centers), color = \"#2DC6D6\") + \n  \n# Formatting\n  \n  labs(title = \"Skree Plot\")\n\n\n\n\n\n\n\n\n6 Application of UMAP\n\numap_results <- stock_date_matrix_tbl %>% \n  select(-symbol) %>% \n  umap()\n\numap_results_tbl <- umap_results$layout %>% \n  as_tibble(.name_repair = \"unique\") %>% \n  set_names(c(\"x\", \"y\")) %>%\n  bind_cols(stock_date_matrix_tbl %>% select(symbol)) \n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\numap_results_tbl %>% \n  ggplot(aes(x, y), ) +\n  geom_point(alpha = 0.5) + \n  theme_tq() + \n  ggrepel::geom_label_repel(aes(label = symbol), size = 3) + \n  labs(title = \"UMAP Projection\")\n\n#> Warning: ggrepel: 500 unlabeled data points (too many overlaps). Consider\n#> increasing max.overlaps\n\n\n\n\n\n\n\n\n\n7 Combination of K-Means and UMAP\n\numap_kmeans_results_tbl <- kmeans_obj %>% \n  augment(stock_date_matrix_tbl) %>% \n  select(symbol, .cluster) %>% \n  left_join(umap_results_tbl) %>% \n  left_join(sp_500_index_tbl %>% select(symbol, company, sector))\n\n#> Joining with `by = join_by(symbol)`\n#> Joining with `by = join_by(symbol)`\n\numap_kmeans_results_tbl %>% \n  ggplot(aes(x, y, color = .cluster)) +\n  \n# Geometries\n\n  geom_point(alpha = 0.5) +\n  ggrepel::geom_label_repel(aes(label = symbol), size = 2, fill = \"#282A36\") +\n  \n# Formatting\n\n  scale_color_manual(values=c(\"#2d72d6\", \"#2dc6d6\", \"#2dd692\", \"#2dd800\")) + \n  theme(legend.position = \"none\")\n\n#> Warning: ggrepel: 497 unlabeled data points (too many overlaps). Consider\n#> increasing max.overlaps"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  },
  {
    "objectID": "content/01_journal/Challenge_2.html",
    "href": "content/01_journal/Challenge_2.html",
    "title": "Challenge 2 - Supervised ML - Regeression",
    "section": "",
    "text": "1 Loading the libraries used for different purposes\n\n# For standard use\n    library(tidyverse)\n\n# For modeling\n    library(parsnip)\n\n# For preprocessing & sampling\n    library(recipes)\n    library(rsample)\n    library(workflows)\n\n# For modeling error metrics\n    library(yardstick)\n\n# Visualization of decision trees\n    library(rpart.plot)\n\n\n2 Preparing the needed data\n\n  bike_orderlines <- read_rds(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/FUNDAMENTAL_DOWNLOADS/bike_orderlines.rds\")\n    \n  neededdata <- bike_orderlines %>% \n\n#Selecting the needed columns\n    \n  select(model, model_year, category_1, category_2, category_3, price, frame_material) %>% \n\n#Removing duplicates and preparing the rows\n    \n  distinct() %>% \n  na.omit() %>% \n  mutate_if(is.character, as.factor)\n  \n  glimpse(neededdata)\n\n#> Rows: 230\n#> Columns: 7\n#> $ model          <fct> Spectral CF 7 WMN, Ultimate CF SLX Disc 8.0 ETAP, Neuro…\n#> $ model_year     <dbl> 2021, 2020, 2021, 2019, 2020, 2020, 2020, 2021, 2020, 2…\n#> $ category_1     <fct> Mountain, Road, Mountain, Road, Mountain, Hybrid / City…\n#> $ category_2     <fct> Trail, Race, Trail, Triathlon Bike, Dirt Jump, City, Tr…\n#> $ category_3     <fct> Spectral, Ultimate, Neuron, Speedmax, Stitched, Roadlit…\n#> $ price          <dbl> 3119, 5359, 2729, 1749, 1219, 1359, 2529, 1559, 3899, 6…\n#> $ frame_material <fct> carbon, carbon, carbon, carbon, aluminium, carbon, carb…\n\n\n\n3 Splitting the data\n\n  set.seed(123)\n  \n  splitted_data <- initial_split(neededdata, prop = 3/4)\n  \n  training_data <- training(splitted_data)\n  testing_data <- testing(splitted_data)\n\n\n4 Using the recipes package\n\n  bikes_recipe <- recipe(price ~ ., data = training_data) %>% \n\n#Adding roles\n  update_role(model, new_role = \"ID\") %>% \n  \n#Adding dummy variables\n  \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors())\n\n\n5 Create Model\n\n  lr_mod <-\n  linear_reg() %>% \n  set_engine(\"glm\") \n\n\n6 Bundle the model and recipe with workflow package\n\n  bikes_workflow <- workflow() %>% \n    add_model(lr_mod) %>% \n    add_recipe(bikes_recipe)\n  bikes_workflow\n\n#> ══ Workflow ════════════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: linear_reg()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 2 Recipe Steps\n#> \n#> • step_dummy()\n#> • step_zv()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> Linear Regression Model Specification (regression)\n#> \n#> Computational engine: glm\n\n\n\n7 Evaluating the model with the yardstick package\n\n#Fit Model\n\n  bikes_fit <- bikes_workflow %>% \n  fit(data = training_data)\n\n#Prediction\n\n  bikes_prediction <- predict(bikes_fit, testing_data) %>% \n  bind_cols(testing_data %>% select(price, model))\n\n#> Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\n#> prediction from a rank-deficient fit may be misleading\n\n  bikes_prediction %>% \n  metrics(truth = price, estimate = .pred)"
  },
  {
    "objectID": "content/01_journal/Challenge_3.html",
    "href": "content/01_journal/Challenge_3.html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "1 Loading required libraries\n\n    library(tidyverse)\n    library(GGally)\n\n    employee_attrition_tbl <- read_csv(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/H2O Data/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n2 Interaction between Monthly Income and Attrition?\n\n# a. Those that are leaving the company have a higher Monthly Income?\n# b. That those are staying have a lower Monthly Income\n# c. Those that are leaving have a lower Monthly Income\n# d. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, MonthlyIncome) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer c. should be correct\n\n3 Interaction between Percent Salary Hike and Attrition?\n\n# a. Those that are leaving the company have a higher Percent Salary Hike\n# b. Those that are staying have a lower Percent Salary Hike\n# c. Those that are leaving have lower Percent Salary Hike\n# d. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, PercentSalaryHike) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer d..\n\n4 Interaction between Stock Option Level and Attrition?\n\n# a. Those that are leaving the company have a higher stock option level\n# b. Those that are staying have a higher stock option level\n# c. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, StockOptionLevel) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer c..\n\n5 Interaction between Environment Satisfaction and Attrition?\n\n# a. A higher proportion of those leaving have a low environment satisfaction level\n# b. A higher proportion of those leaving have a high environment satisfaction level\n# c. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, EnvironmentSatisfaction) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer a..\n\n6 Interaction between Work Life Balance and Attrition?\n\n# a. Those that are leaving have higher density of 2's and 3's\n# b. Those that are staying have a higher density of 2's and 3's\n# c. Those that are staying have a lower density of 2's and 3's\n# d. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, WorkLifeBalance) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer b..\n\n7 Interaction between Job Involvement and Attrition?\n\n# a. Those that are leaving have a lower density of 3's and 4's\n# b. Those that are leaving have a lower density of 1's and 2's\n# c. Those that are staying have a lower density of 2's and 3's\n# d. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, JobInvolvement) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer a..\n\n8 Interaction between Over Time and Attrition?\n\n# a. The proportion of those leaving that are working Over Time are high compared to those that are not leaving\n# b. The proportion of those staying that are working Over Time are high compared to those that are not staying\n\nemployee_attrition_tbl %>% \n  select(Attrition, OverTime) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer b..\n\n9 Interaction between Training Times Last Year and Attrition?\n\n# a. People that leave tend to have more annual trainings\n# b. People that leave tend to have less annual trainings\n# c. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, TrainingTimesLastYear) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer c..\n\n10 Interaction between Years At Company and Attrition?\n\n# a. People that leave tend to have more working years at the company\n# b. People that leave tend to have less working years at the company\n# c. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, YearsAtCompany) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer b..\n\n11 Interaction between Years Since Last Promotion and Attrition?\n\n# a. Those that are leaving have more years since last promotion than those that are staying\n# b. Those that are leaving have fewer years since last promotion than those that are staying\n# c. It's difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, YearsSinceLastPromotion) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nAnswer c.."
  },
  {
    "objectID": "content/01_journal/Challenge_3.html#those-that-are-leaving-the-company-have-a-higher-monthly-income",
    "href": "content/01_journal/Challenge_3.html#those-that-are-leaving-the-company-have-a-higher-monthly-income",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "\n2.1 Those that are leaving the company have a higher Monthly Income?",
    "text": "2.1 Those that are leaving the company have a higher Monthly Income?"
  },
  {
    "objectID": "content/01_journal/Challenge_3.html#that-those-are-staying-have-a-lower-monthly-income",
    "href": "content/01_journal/Challenge_3.html#that-those-are-staying-have-a-lower-monthly-income",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "\n2.2 That those are staying have a lower Monthly Income",
    "text": "2.2 That those are staying have a lower Monthly Income"
  },
  {
    "objectID": "content/01_journal/Challenge_3.html#those-that-are-leaving-have-a-lower-monthly-income",
    "href": "content/01_journal/Challenge_3.html#those-that-are-leaving-have-a-lower-monthly-income",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "\n2.3 Those that are leaving have a lower Monthly Income",
    "text": "2.3 Those that are leaving have a lower Monthly Income"
  },
  {
    "objectID": "content/01_journal/Challenge_3.html#its-difficult-to-deduce-anything-based-on-the-visualization",
    "href": "content/01_journal/Challenge_3.html#its-difficult-to-deduce-anything-based-on-the-visualization",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "\n2.4 It’s difficult to deduce anything based on the visualization",
    "text": "2.4 It’s difficult to deduce anything based on the visualization\n\nemployee_attrition_tbl %>% \n  select(Attrition, MonthlyIncome) %>% \n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n            diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nWe can deduce that those that are leaving are more likely to be the ones with lower income as seen in the upper right plot. Answer: C"
  },
  {
    "objectID": "content/01_journal/Challenge_3.html#monthly-income",
    "href": "content/01_journal/Challenge_3.html#monthly-income",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "\n2.5 Monthly Income",
    "text": "2.5 Monthly Income\n\nemployee_attrition_tbl %>% \n  select(Attrition, MonthlyIncome) %>% \n  ggpairs()\n\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "content/01_journal/Challenge_4.html",
    "href": "content/01_journal/Challenge_4.html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "1 Steps for this challenge\n\n# 1. Load the training & test dataset\n# 2. Specifiy the response and predictor variables\n# 3. run AutoML specifying the stopping criterion\n# 4. View the leaderboard\n# 5. Predicting using Leader Model\n# 6. Save the leader model\n\n\n2 Loading required libraries\n\nlibrary(tidyverse)\nlibrary(h2o)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(cowplot)\nlibrary(glue)\n\n\n3 Loading required data\n\n    product_backorders <- read_csv(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/H2O Data/product_backorders.csv\")\n\n#> Rows: 19053 Columns: 23\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#> dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n4 Splitting the data to training and test\n\nset.seed(1234)\nsplit_obj <- initial_split(product_backorders, prop = 0.85)\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl <- testing(split_obj)\n\nproduct_rec <- recipe(went_on_backorder ~ ., data = train_readable_tbl) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  prep()\n\ntrain_data <- bake(product_rec, new_data = train_readable_tbl)\ntest_data <- bake(product_rec, new_data = test_readable_tbl)\n\n\n5 Response and predictor variables\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         18 hours 3 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 9 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.59 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nsplit_h2o <- h2o.splitFrame(as.h2o(train_data), ratios = c(0.85), seed = 1234)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_data)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o),y)\n\n\n6 Running AutoML\n\nautoml_models_h2o <- h2o.automl(\n   x = x,\n   y = y,\n   training_frame    = train_h2o,\n   validation_frame  = valid_h2o,\n   leaderboard_frame = test_h2o,\n   max_runtime_secs  = 30,\n   nfolds            = 5 \n )\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n#> 14:18:44.231: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n7 Leaderboard and leadermodel\n\ntypeof(automl_models_h2o)\n\n#> [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#> [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#> [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard\n\n#>                                                  model_id       auc   logloss\n#> 1 StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844 0.9527393 0.1733521\n#> 2 StackedEnsemble_BestOfFamily_1_AutoML_2_20230606_141844 0.9524164 0.1736103\n#> 3                      XGBoost_1_AutoML_2_20230606_141844 0.9485036 0.1804360\n#> 4                          GBM_1_AutoML_2_20230606_141844 0.9484397 0.1837795\n#> 5                      XGBoost_2_AutoML_2_20230606_141844 0.9393281 0.2191810\n#> 6                          GBM_4_AutoML_2_20230606_141844 0.9030596 0.3000969\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7553282            0.1497764 0.2288331 0.05236458\n#> 2 0.7543424            0.1534461 0.2289184 0.05240364\n#> 3 0.7389788            0.1455271 0.2322649 0.05394698\n#> 4 0.7385847            0.1601098 0.2358665 0.05563301\n#> 5 0.7265648            0.1690076 0.2419052 0.05851811\n#> 6 0.6108258            0.1913555 0.2969982 0.08820794\n#> \n#> [10 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844 \n#> Model Summary for Stacked Ensemble: \n#>                                     key            value\n#> 1                     Stacking strategy cross_validation\n#> 2  Number of base models (used / total)              4/4\n#> 3      # GBM base models (used / total)              1/1\n#> 4  # XGBoost base models (used / total)              1/1\n#> 5      # DRF base models (used / total)              1/1\n#> 6      # GLM base models (used / total)              1/1\n#> 7                 Metalearner algorithm              GLM\n#> 8    Metalearner fold assignment scheme           Random\n#> 9                    Metalearner nfolds                5\n#> 10              Metalearner fold_column               NA\n#> 11   Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.03371848\n#> RMSE:  0.1836259\n#> LogLoss:  0.1193677\n#> Mean Per-Class Error:  0.09899042\n#> AUC:  0.9798214\n#> AUCPR:  0.8915564\n#> Gini:  0.9596428\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error       Rate\n#> No     8545  232 0.026433  =232/8777\n#> Yes     205  990 0.171548  =205/1195\n#> Totals 8750 1222 0.043823  =437/9972\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.397876    0.819197 181\n#> 2                       max f2  0.233428    0.858491 231\n#> 3                 max f0point5  0.578893    0.843895 130\n#> 4                 max accuracy  0.493223    0.957280 155\n#> 5                max precision  0.980361    1.000000   0\n#> 6                   max recall  0.010346    1.000000 371\n#> 7              max specificity  0.980361    1.000000   0\n#> 8             max absolute_mcc  0.397876    0.794332 181\n#> 9   max min_per_class_accuracy  0.179875    0.927310 252\n#> 10 max mean_per_class_accuracy  0.225628    0.929334 234\n#> 11                     max tns  0.980361 8777.000000   0\n#> 12                     max fns  0.980361 1191.000000   0\n#> 13                     max fps  0.000735 8777.000000 399\n#> 14                     max tps  0.010346 1195.000000 371\n#> 15                     max tnr  0.980361    1.000000   0\n#> 16                     max fnr  0.980361    0.996653   0\n#> 17                     max fpr  0.000735    1.000000 399\n#> 18                     max tpr  0.010346    1.000000 371\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.04474767\n#> RMSE:  0.2115364\n#> LogLoss:  0.1510496\n#> Mean Per-Class Error:  0.1342284\n#> AUC:  0.9589127\n#> AUCPR:  0.7773943\n#> Gini:  0.9178254\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2026 103 0.048380  =103/2129\n#> Yes      57 202 0.220077    =57/259\n#> Totals 2083 305 0.067002  =160/2388\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.349697    0.716312 176\n#> 2                       max f2  0.202734    0.781359 223\n#> 3                 max f0point5  0.656637    0.743455  96\n#> 4                 max accuracy  0.508223    0.939280 128\n#> 5                max precision  0.970273    1.000000   0\n#> 6                   max recall  0.006515    1.000000 378\n#> 7              max specificity  0.970273    1.000000   0\n#> 8             max absolute_mcc  0.311023    0.682403 186\n#> 9   max min_per_class_accuracy  0.152082    0.895753 247\n#> 10 max mean_per_class_accuracy  0.116897    0.902780 266\n#> 11                     max tns  0.970273 2129.000000   0\n#> 12                     max fns  0.970273  258.000000   0\n#> 13                     max fps  0.000553 2129.000000 399\n#> 14                     max tps  0.006515  259.000000 378\n#> 15                     max tnr  0.970273    1.000000   0\n#> 16                     max fnr  0.970273    0.996139   0\n#> 17                     max fpr  0.000553    1.000000 399\n#> 18                     max tpr  0.006515    1.000000 378\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.05118106\n#> RMSE:  0.2262323\n#> LogLoss:  0.1727723\n#> Mean Per-Class Error:  0.1520616\n#> AUC:  0.9490955\n#> AUCPR:  0.7460987\n#> Gini:  0.898191\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     11543  610 0.050193   =610/12153\n#> Yes      420 1234 0.253930    =420/1654\n#> Totals 11963 1844 0.074600  =1030/13807\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.336672     0.705546 200\n#> 2                       max f2  0.173934     0.773829 260\n#> 3                 max f0point5  0.606201     0.726455 119\n#> 4                 max accuracy  0.465607     0.930470 159\n#> 5                max precision  0.981631     1.000000   0\n#> 6                   max recall  0.001737     1.000000 396\n#> 7              max specificity  0.981631     1.000000   0\n#> 8             max absolute_mcc  0.336672     0.664265 200\n#> 9   max min_per_class_accuracy  0.121191     0.882498 285\n#> 10 max mean_per_class_accuracy  0.113819     0.884889 289\n#> 11                     max tns  0.981631 12153.000000   0\n#> 12                     max fns  0.981631  1649.000000   0\n#> 13                     max fps  0.000612 12153.000000 399\n#> 14                     max tps  0.001737  1654.000000 396\n#> 15                     max tnr  0.981631     1.000000   0\n#> 16                     max fnr  0.981631     0.996977   0\n#> 17                     max fpr  0.000612     1.000000 399\n#> 18                     max tpr  0.001737     1.000000 396\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.928868  0.007200   0.925966   0.919870   0.925993   0.935747\n#> auc         0.947436  0.005531   0.938772   0.951460   0.950667   0.945005\n#> err         0.071132  0.007200   0.074034   0.080131   0.074007   0.064253\n#> err_count 196.400000 19.718012 203.000000 221.000000 205.000000 181.000000\n#> f0point5    0.700417  0.028890   0.701811   0.662437   0.683263   0.736111\n#>           cv_5_valid\n#> accuracy    0.936765\n#> auc         0.951276\n#> err         0.063235\n#> err_count 172.000000\n#> f0point5    0.718466\n#> \n#> ---\n#>                          mean        sd  cv_1_valid  cv_2_valid  cv_3_valid\n#> precision            0.693022  0.040015    0.716172    0.638142    0.663239\n#> r2                   0.468373  0.027896    0.436864    0.457532    0.472092\n#> recall               0.738393  0.054802    0.649701    0.781437    0.777108\n#> residual_deviance 1068.577800 99.574530 1135.221800 1088.680000 1075.436500\n#> rmse                 0.236698  0.009607    0.245437    0.240288    0.235985\n#> specificity          0.954753  0.011494    0.964286    0.938944    0.946267\n#>                    cv_4_valid cv_5_valid\n#> precision            0.730028   0.717532\n#> r2                   0.462766   0.512612\n#> recall               0.761494   0.722222\n#> residual_deviance 1145.036700 898.513700\n#> rmse                 0.241182   0.220596\n#> specificity          0.960308   0.963960\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         18 hours 59 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 9 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.61 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\n# h2o.getModel(\"StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\") %>%\n# h2o.saveModel(path = \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test\")\n\n\n8 Predicting using leader model\n\nstacked_ensemble_h2o <- h2o.loadModel(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\")\n\npredictions <- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_data))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#> [1] \"environment\"\n\npredictions_tbl <- predictions %>% as_tibble()\n\n\n9 Optional: Preparation for recreating the model and/or tune some values\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         19 hours 2 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 9 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.61 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\ndeep_learning_h2o <- h2o.loadModel(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\")\n\n?h2o.deeplearning\n\ndeep_learning_h2o@allparameters\n\n#> $model_id\n#> [1] \"StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\"\n#> \n#> $training_frame\n#> [1] \"AutoML_2_20230606_141844_training_RTMP_sid_b202_5\"\n#> \n#> $base_models\n#> $base_models[[1]]\n#> $base_models[[1]]$`__meta`\n#> $base_models[[1]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[1]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[1]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[1]]$name\n#> [1] \"XGBoost_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[1]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[1]]$URL\n#> NULL\n#> \n#> \n#> $base_models[[2]]\n#> $base_models[[2]]$`__meta`\n#> $base_models[[2]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[2]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[2]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[2]]$name\n#> [1] \"GBM_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[2]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[2]]$URL\n#> NULL\n#> \n#> \n#> $base_models[[3]]\n#> $base_models[[3]]$`__meta`\n#> $base_models[[3]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[3]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[3]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[3]]$name\n#> [1] \"DRF_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[3]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[3]]$URL\n#> NULL\n#> \n#> \n#> $base_models[[4]]\n#> $base_models[[4]]$`__meta`\n#> $base_models[[4]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[4]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[4]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[4]]$name\n#> [1] \"GLM_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[4]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[4]]$URL\n#> NULL\n#> \n#> \n#> \n#> $metalearner_algorithm\n#> [1] \"glm\"\n#> \n#> $metalearner_nfolds\n#> [1] 5\n#> \n#> $metalearner_params\n#> [1] \"\"\n#> \n#> $metalearner_transform\n#> [1] \"Logit\"\n#> \n#> $max_runtime_secs\n#> [1] 0.4566667\n#> \n#> $seed\n#> [1] \"5766883882285979032\"\n#> \n#> $score_training_samples\n#> [1] 10000\n#> \n#> $keep_levelone_frame\n#> [1] TRUE\n#> \n#> $auc_type\n#> [1] \"AUTO\"\n#> \n#> $x\n#>  [1] \"sku\"                 \"national_inv\"        \"lead_time\"          \n#>  [4] \"in_transit_qty\"      \"forecast_3_month\"    \"forecast_6_month\"   \n#>  [7] \"forecast_9_month\"    \"sales_1_month\"       \"sales_3_month\"      \n#> [10] \"sales_6_month\"       \"sales_9_month\"       \"min_bank\"           \n#> [13] \"pieces_past_due\"     \"perf_6_month_avg\"    \"perf_12_month_avg\"  \n#> [16] \"local_bo_qty\"        \"potential_issue_Yes\" \"deck_risk_Yes\"      \n#> [19] \"oe_constraint_Yes\"   \"ppap_risk_Yes\"       \"stop_auto_buy_Yes\"  \n#> [22] \"rev_stop_Yes\"       \n#> \n#> $y\n#> [1] \"went_on_backorder\""
  },
  {
    "objectID": "content/01_journal/Challenge_5.html",
    "href": "content/01_journal/Challenge_5.html",
    "title": "Performance Measures",
    "section": "",
    "text": "Code of Challenge 4\n\nApply all the steps you have learned in this session on the dataset from challenge of the last session (Product Backorders):\n\nLeaderboard visualization\nTune a model with grid search\nVisualize the trade of between the precision and the recall and the optimal threshold\nROC Plot\nPrecision vs Recall Plot\nGain Plot\nLift Plot\nDashboard with cowplot\n\n\n\n\n\n\n\nNote\n\n\n\nBetween this and the next callout-note is the same code as used in challenge 4\n\n\n2 Loading required libraries\n\nlibrary(tidyverse)\nlibrary(h2o)\nlibrary(rsample)\nlibrary(recipes)\nlibrary(cowplot)\n\n#> \n#> Attaching package: 'cowplot'\n\n\n#> The following object is masked from 'package:lubridate':\n#> \n#>     stamp\n\nlibrary(glue)\n\n3 Loading required data\n\n    product_backorders <- read_csv(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/H2O Data/product_backorders.csv\")\n\n#> Rows: 19053 Columns: 23\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (7): potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_bu...\n#> dbl (16): sku, national_inv, lead_time, in_transit_qty, forecast_3_month, fo...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n4 Splitting the data to training and test\n\nset.seed(1234)\nsplit_obj <- initial_split(product_backorders, prop = 0.85)\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl <- testing(split_obj)\n\nproduct_rec <- recipe(went_on_backorder ~ ., data = train_readable_tbl) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  prep()\n\ntrain_data <- bake(product_rec, new_data = train_readable_tbl)\ntest_data <- bake(product_rec, new_data = test_readable_tbl)\n\n5 Response and predictor variables\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 days 20 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.62 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nsplit_h2o <- h2o.splitFrame(as.h2o(train_data), ratios = c(0.85), seed = 1234)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_data)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o),y)\n\n6 Running AutoML\n\nautoml_models_h2o <- h2o.automl(\n   x = x,\n   y = y,\n   training_frame    = train_h2o,\n   validation_frame  = valid_h2o,\n   leaderboard_frame = test_h2o,\n   max_runtime_secs  = 30,\n   nfolds            = 5 \n )\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n#> 16:17:08.949: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |======================================================================| 100%\n\n\n7 Leaderboard and leadermodel\n\ntypeof(automl_models_h2o)\n\n#> [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#> [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#> [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard\n\n#>                                                  model_id       auc   logloss\n#> 1 StackedEnsemble_BestOfFamily_1_AutoML_3_20230607_161708 0.9536553 0.1724271\n#> 2 StackedEnsemble_BestOfFamily_2_AutoML_3_20230607_161708 0.9534280 0.1728308\n#> 3                          GBM_1_AutoML_3_20230607_161708 0.9502559 0.1831675\n#> 4                      XGBoost_1_AutoML_3_20230607_161708 0.9495807 0.1786639\n#> 5                      XGBoost_2_AutoML_3_20230607_161708 0.9384116 0.2176056\n#> 6                          DRF_1_AutoML_3_20230607_161708 0.8987340 0.2811616\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7661186            0.1717562 0.2284561 0.05219220\n#> 2 0.7676726            0.1672881 0.2283156 0.05212802\n#> 3 0.7386430            0.1582557 0.2362444 0.05581141\n#> 4 0.7564656            0.1487976 0.2309446 0.05333541\n#> 5 0.7309856            0.1777058 0.2410858 0.05812235\n#> 6 0.6156206            0.2110244 0.2650532 0.07025321\n#> \n#> [10 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_BestOfFamily_1_AutoML_3_20230607_161708 \n#> Model Summary for Stacked Ensemble: \n#>                                     key            value\n#> 1                     Stacking strategy cross_validation\n#> 2  Number of base models (used / total)              3/3\n#> 3      # GBM base models (used / total)              1/1\n#> 4  # XGBoost base models (used / total)              1/1\n#> 5      # GLM base models (used / total)              1/1\n#> 6                 Metalearner algorithm              GLM\n#> 7    Metalearner fold assignment scheme           Random\n#> 8                    Metalearner nfolds                5\n#> 9               Metalearner fold_column               NA\n#> 10   Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.0331494\n#> RMSE:  0.1820698\n#> LogLoss:  0.1184036\n#> Mean Per-Class Error:  0.1086234\n#> AUC:  0.9805972\n#> AUCPR:  0.8963297\n#> Gini:  0.9611944\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error       Rate\n#> No     8614  175 0.019911  =175/8789\n#> Yes     237  964 0.197336  =237/1201\n#> Totals 8851 1139 0.041241  =412/9990\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.452641    0.823932 168\n#> 2                       max f2  0.257482    0.859400 225\n#> 3                 max f0point5  0.577935    0.851064 133\n#> 4                 max accuracy  0.452641    0.958759 168\n#> 5                max precision  0.985164    1.000000   0\n#> 6                   max recall  0.010651    1.000000 371\n#> 7              max specificity  0.985164    1.000000   0\n#> 8             max absolute_mcc  0.452641    0.800955 168\n#> 9   max min_per_class_accuracy  0.187761    0.929685 250\n#> 10 max mean_per_class_accuracy  0.194759    0.930518 247\n#> 11                     max tns  0.985164 8789.000000   0\n#> 12                     max fns  0.985164 1199.000000   0\n#> 13                     max fps  0.000607 8789.000000 399\n#> 14                     max tps  0.010651 1201.000000 371\n#> 15                     max tnr  0.985164    1.000000   0\n#> 16                     max fnr  0.985164    0.998335   0\n#> 17                     max fpr  0.000607    1.000000 399\n#> 18                     max tpr  0.010651    1.000000 371\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.04443069\n#> RMSE:  0.2107859\n#> LogLoss:  0.1501155\n#> Mean Per-Class Error:  0.1359757\n#> AUC:  0.9590868\n#> AUCPR:  0.7864287\n#> Gini:  0.9181736\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2035  94 0.044152   =94/2129\n#> Yes      59 200 0.227799    =59/259\n#> Totals 2094 294 0.064070  =153/2388\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.369012    0.723327 175\n#> 2                       max f2  0.195185    0.792768 229\n#> 3                 max f0point5  0.664474    0.745659 104\n#> 4                 max accuracy  0.524776    0.939280 139\n#> 5                max precision  0.971322    1.000000   0\n#> 6                   max recall  0.006250    1.000000 379\n#> 7              max specificity  0.971322    1.000000   0\n#> 8             max absolute_mcc  0.369012    0.689027 175\n#> 9   max min_per_class_accuracy  0.143159    0.892907 252\n#> 10 max mean_per_class_accuracy  0.117526    0.900850 265\n#> 11                     max tns  0.971322 2129.000000   0\n#> 12                     max fns  0.971322  257.000000   0\n#> 13                     max fps  0.000494 2129.000000 399\n#> 14                     max tps  0.006250  259.000000 379\n#> 15                     max tnr  0.971322    1.000000   0\n#> 16                     max fnr  0.971322    0.992278   0\n#> 17                     max fpr  0.000494    1.000000 399\n#> 18                     max tpr  0.006250    1.000000 379\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.05058479\n#> RMSE:  0.2249106\n#> LogLoss:  0.1714974\n#> Mean Per-Class Error:  0.1505787\n#> AUC:  0.9497005\n#> AUCPR:  0.7475923\n#> Gini:  0.899401\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     11557  596 0.049041   =596/12153\n#> Yes      417 1237 0.252116    =417/1654\n#> Totals 11974 1833 0.073369  =1013/13807\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.329871     0.709492 209\n#> 2                       max f2  0.170900     0.777437 268\n#> 3                 max f0point5  0.599109     0.737433 126\n#> 4                 max accuracy  0.539519     0.931774 144\n#> 5                max precision  0.988358     1.000000   0\n#> 6                   max recall  0.002198     1.000000 395\n#> 7              max specificity  0.988358     1.000000   0\n#> 8             max absolute_mcc  0.329871     0.668787 209\n#> 9   max min_per_class_accuracy  0.124049     0.883313 290\n#> 10 max mean_per_class_accuracy  0.100891     0.885585 302\n#> 11                     max tns  0.988358 12153.000000   0\n#> 12                     max fns  0.988358  1653.000000   0\n#> 13                     max fps  0.000647 12153.000000 399\n#> 14                     max tps  0.002198  1654.000000 395\n#> 15                     max tnr  0.988358     1.000000   0\n#> 16                     max fnr  0.988358     0.999395   0\n#> 17                     max fpr  0.000647     1.000000 399\n#> 18                     max tpr  0.002198     1.000000 395\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.929452  0.006466   0.939790   0.924468   0.928366   0.930872\n#> auc         0.950070  0.002367   0.952007   0.951888   0.948251   0.951357\n#> err         0.070548  0.006466   0.060210   0.075532   0.071634   0.069128\n#> err_count 194.800000 18.005554 166.000000 213.000000 199.000000 191.000000\n#> f0point5    0.703828  0.027116   0.750514   0.695489   0.682081   0.689158\n#>           cv_5_valid\n#> accuracy    0.923763\n#> auc         0.946846\n#> err         0.076237\n#> err_count 205.000000\n#> f0point5    0.701897\n#> \n#> ---\n#>                         mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> precision           0.697946  0.040590   0.768421   0.687003   0.668555\n#> r2                  0.520437  0.019085   0.551716   0.519597   0.502099\n#> recall              0.732713  0.027932   0.686520   0.731638   0.742138\n#> residual_deviance 945.659500 46.546272 881.382200 998.188000 947.736700\n#> rmse                0.224748  0.007168   0.214167   0.229642   0.224657\n#> specificity         0.956201  0.009377   0.972929   0.952149   0.952439\n#>                   cv_4_valid cv_5_valid\n#> precision           0.673239   0.692513\n#> r2                  0.508538   0.520234\n#> recall              0.761146   0.742120\n#> residual_deviance 921.132750 979.857900\n#> rmse                0.222497   0.232779\n#> specificity         0.952634   0.950855\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 days 20 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.60 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\n# h2o.getModel(\"StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\") %>%\n# h2o.saveModel(path = \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test\")\n\n8 Predicting using leader model\n\nstacked_ensemble_h2o <- h2o.loadModel(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\")\n\npredictions <- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_data))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#> [1] \"environment\"\n\npredictions_tbl <- predictions %>% as_tibble()\n\n9 Optional: Preparation for recreating the model and/or tune some values\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 days 20 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.60 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\ndeep_learning_h2o <- h2o.loadModel(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\")\n\n?h2o.deeplearning\n\ndeep_learning_h2o@allparameters\n\n#> $model_id\n#> [1] \"StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\"\n#> \n#> $training_frame\n#> [1] \"AutoML_2_20230606_141844_training_RTMP_sid_b202_5\"\n#> \n#> $base_models\n#> $base_models[[1]]\n#> $base_models[[1]]$`__meta`\n#> $base_models[[1]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[1]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[1]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[1]]$name\n#> [1] \"XGBoost_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[1]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[1]]$URL\n#> NULL\n#> \n#> \n#> $base_models[[2]]\n#> $base_models[[2]]$`__meta`\n#> $base_models[[2]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[2]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[2]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[2]]$name\n#> [1] \"GBM_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[2]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[2]]$URL\n#> NULL\n#> \n#> \n#> $base_models[[3]]\n#> $base_models[[3]]$`__meta`\n#> $base_models[[3]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[3]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[3]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[3]]$name\n#> [1] \"DRF_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[3]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[3]]$URL\n#> NULL\n#> \n#> \n#> $base_models[[4]]\n#> $base_models[[4]]$`__meta`\n#> $base_models[[4]]$`__meta`$schema_version\n#> [1] 3\n#> \n#> $base_models[[4]]$`__meta`$schema_name\n#> [1] \"KeyV3\"\n#> \n#> $base_models[[4]]$`__meta`$schema_type\n#> [1] \"Key<Keyed>\"\n#> \n#> \n#> $base_models[[4]]$name\n#> [1] \"GLM_1_AutoML_2_20230606_141844\"\n#> \n#> $base_models[[4]]$type\n#> [1] \"Key<Keyed>\"\n#> \n#> $base_models[[4]]$URL\n#> NULL\n#> \n#> \n#> \n#> $metalearner_algorithm\n#> [1] \"glm\"\n#> \n#> $metalearner_nfolds\n#> [1] 5\n#> \n#> $metalearner_params\n#> [1] \"\"\n#> \n#> $metalearner_transform\n#> [1] \"Logit\"\n#> \n#> $max_runtime_secs\n#> [1] 0.4566667\n#> \n#> $seed\n#> [1] \"5766883882285979032\"\n#> \n#> $score_training_samples\n#> [1] 10000\n#> \n#> $keep_levelone_frame\n#> [1] TRUE\n#> \n#> $auc_type\n#> [1] \"AUTO\"\n#> \n#> $x\n#>  [1] \"sku\"                 \"national_inv\"        \"lead_time\"          \n#>  [4] \"in_transit_qty\"      \"forecast_3_month\"    \"forecast_6_month\"   \n#>  [7] \"forecast_9_month\"    \"sales_1_month\"       \"sales_3_month\"      \n#> [10] \"sales_6_month\"       \"sales_9_month\"       \"min_bank\"           \n#> [13] \"pieces_past_due\"     \"perf_6_month_avg\"    \"perf_12_month_avg\"  \n#> [16] \"local_bo_qty\"        \"potential_issue_Yes\" \"deck_risk_Yes\"      \n#> [19] \"oe_constraint_Yes\"   \"ppap_risk_Yes\"       \"stop_auto_buy_Yes\"  \n#> [22] \"rev_stop_Yes\"       \n#> \n#> $y\n#> [1] \"went_on_backorder\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nEnd of challenge 4 code\n\n\nApply all the steps you have learned in this session on the dataset from challenge of the last session (Product Backorders):\n\nLeaderboard visualization\nTune a model with grid search\nVisualize the trade of between the precision and the recall and the optimal threshold\nROC Plot\nPrecision vs Recall Plot\nGain Plot\nLift Plot\nDashboard with cowplot\n\nQuick Preparation:\n\nlibrary(tidyverse)\nlibrary(h2o)\nlibrary(rsample)\nlibrary(recipes)\n\n# The 3 top models\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 16 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.60 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nh2o.getModel(\"StackedEnsemble_BestOfFamily_1_AutoML_3_20230607_161708\") %>%\nh2o.saveModel(path = \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test\")\n\n#> [1] \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_1_AutoML_3_20230607_161708\"\n\nh2o.getModel(\"StackedEnsemble_BestOfFamily_2_AutoML_3_20230607_161708\") %>%\nh2o.saveModel(path = \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test\")\n\n#> [1] \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_2_AutoML_3_20230607_161708\"\n\nh2o.getModel(\"GBM_1_AutoML_3_20230607_161708\") %>%\nh2o.saveModel(path = \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test\")\n\n#> [1] \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/GBM_1_AutoML_3_20230607_161708\"\n\n\n\n1 Leaderboard visualization\n\ndata_transformed_tbl <- automl_models_h2o@leaderboard %>%\n        as_tibble() %>%\n        select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n        mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n        slice(1:15) %>% \n        rownames_to_column(var = \"rowname\") %>%\n\n        mutate(\n          model_id   = as_factor(model_id) %>% reorder(auc),\n          model_type = as.factor(model_type)\n          ) %>% \n          pivot_longer(cols = -c(model_id, model_type, rowname), \n                       names_to = \"key\", \n                       values_to = \"value\", \n                       names_transform = list(key = forcats::fct_inorder)\n                       ) %>% \n        mutate(model_id = paste0(rowname, \". \", model_id) %>% as_factor() %>% fct_rev())\n\ndata_transformed_tbl %>%\n        ggplot(aes(value, model_id, color = model_type)) +\n        geom_point(size = 3) +\n        geom_label(aes(label = round(value, 2), hjust = \"inward\")) +\n        \n\n        facet_wrap(~ key, scales = \"free_x\") +\n        labs(title = \"Leaderboard Metrics\",\n             subtitle = paste0(\"Ordered by: \", \"auc\"),\n             y = \"Model Postion, Model ID\", x = \"\") + \n        theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n2 Tune a model with grid search\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 13 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.56 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nmodel1 <- h2o.loadModel(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\")\n\nh2o.performance(model1, newdata = as.h2o(test_data))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n#> H2OBinomialMetrics: stackedensemble\n#> \n#> MSE:  0.05236458\n#> RMSE:  0.2288331\n#> LogLoss:  0.1733521\n#> Mean Per-Class Error:  0.1497764\n#> AUC:  0.9527393\n#> AUCPR:  0.7553282\n#> Gini:  0.9054786\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2372 133 0.053094  =133/2505\n#> Yes      87 266 0.246459    =87/353\n#> Totals 2459 399 0.076977  =220/2858\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.318209    0.707447 193\n#> 2                       max f2  0.140659    0.789474 264\n#> 3                 max f0point5  0.491648    0.731148 138\n#> 4                 max accuracy  0.491648    0.930021 138\n#> 5                max precision  0.978623    1.000000   0\n#> 6                   max recall  0.008918    1.000000 377\n#> 7              max specificity  0.978623    1.000000   0\n#> 8             max absolute_mcc  0.318209    0.664968 193\n#> 9   max min_per_class_accuracy  0.140659    0.892351 264\n#> 10 max mean_per_class_accuracy  0.129535    0.893758 271\n#> 11                     max tns  0.978623 2505.000000   0\n#> 12                     max fns  0.978623  352.000000   0\n#> 13                     max fps  0.000749 2505.000000 399\n#> 14                     max tps  0.008918  353.000000 377\n#> 15                     max tnr  0.978623    1.000000   0\n#> 16                     max fnr  0.978623    0.997167   0\n#> 17                     max fpr  0.000749    1.000000 399\n#> 18                     max tpr  0.008918    1.000000 377\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\ndeeplearning_grid_01 <- h2o.grid(\n  algorithm = \"deeplearning\",\n  grid_id = \"deeplearning_grid_01\",\n  x = x,\n  y = y,\n  training_frame   = train_h2o,\n  validation_frame = valid_h2o,\n  nfolds = 5,\n  hyper_params = list(\n    hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n    epochs = c(10, 50, 100)\n  )\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n#> Warning in h2o.getGrid(grid_id = grid_id): Some models were not built due to a\n#> failure, for more details run `summary(grid_object, show_stack_traces = TRUE)`\n\ndeeplearning_grid_01_model_1 <- h2o.getModel(\"StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\")\ndeeplearning_grid_01_model_1 %>% h2o.auc(train = T, valid = T, xval = T)\n\n#>     train     valid      xval \n#> 0.9798214 0.9589127 0.9490955\n\ndeeplearning_grid_01_model_1 %>%\n  h2o.performance(newdata = as.h2o(test_data))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n#> H2OBinomialMetrics: stackedensemble\n#> \n#> MSE:  0.05236458\n#> RMSE:  0.2288331\n#> LogLoss:  0.1733521\n#> Mean Per-Class Error:  0.1497764\n#> AUC:  0.9527393\n#> AUCPR:  0.7553282\n#> Gini:  0.9054786\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2372 133 0.053094  =133/2505\n#> Yes      87 266 0.246459    =87/353\n#> Totals 2459 399 0.076977  =220/2858\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.318209    0.707447 193\n#> 2                       max f2  0.140659    0.789474 264\n#> 3                 max f0point5  0.491648    0.731148 138\n#> 4                 max accuracy  0.491648    0.930021 138\n#> 5                max precision  0.978623    1.000000   0\n#> 6                   max recall  0.008918    1.000000 377\n#> 7              max specificity  0.978623    1.000000   0\n#> 8             max absolute_mcc  0.318209    0.664968 193\n#> 9   max min_per_class_accuracy  0.140659    0.892351 264\n#> 10 max mean_per_class_accuracy  0.129535    0.893758 271\n#> 11                     max tns  0.978623 2505.000000   0\n#> 12                     max fns  0.978623  352.000000   0\n#> 13                     max fps  0.000749 2505.000000 399\n#> 14                     max tps  0.008918  353.000000 377\n#> 15                     max tnr  0.978623    1.000000   0\n#> 16                     max fnr  0.978623    0.997167   0\n#> 17                     max fpr  0.000749    1.000000 399\n#> 18                     max tpr  0.008918    1.000000 377\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\ntheme_new <- theme(\n  legend.position  = \"bottom\",\n  legend.key       = element_blank(),\n  panel.background = element_rect(fill   = \"transparent\"),\n  panel.border     = element_rect(color = \"black\", fill = NA, linewidth = 0.5),\n  panel.grid.major = element_line(color = \"grey\", linewidth = 0.333)\n) \n\n\n3 Visualize the trade of between the precision and the recall and the optimal threshold\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 16 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.60 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nperformance_h2o <- h2o.performance(model1, newdata = as.h2o(test_data))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nperformance_tbl <- performance_h2o %>%\n  h2o.metric() %>%\n  as_tibble() \n\nperformance_tbl %>%\n  filter(f1 == max(f1))\n\n\n\n  \n\n\nperformance_tbl %>%\n  ggplot(aes(x = threshold)) +\n  geom_line(aes(y = precision), color = \"red\", linewidth = 1.1) +\n  geom_line(aes(y = recall), color = \"blue\", linewidth = 1.1) +\n  \n  # Inserting line where precision and recall are harmonically optimized\n  \n  geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n  labs(title = \"Precision (red) vs Recall (blue)\", y = \"value\") +\n  theme_new\n\n\n\n\n\n\n\n\n4 ROC Plot\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 16 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.59 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\npath <- \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test/StackedEnsemble_BestOfFamily_2_AutoML_2_20230606_141844\"\nload_model_performance_metrics <- function(path, test_data) {\n  path\n  model_h2o <- h2o.loadModel(path)\n  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_data)) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as_tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc, recall, precision)\n  \n}\n\n\nmodel_metrics_tbl <- fs::dir_info(path = \"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/test\") %>%\n  select(path) %>%\n  mutate(metrics = map(path, load_model_performance_metrics, test_data)) %>%\n  unnest(cols = metrics)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nmodel_metrics_tbl %>%\n    mutate(\n      path = str_split(path, pattern = \"/\", simplify = T)[,2] %>% as_factor(),\n      auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n    ) %>%\n  ggplot(aes(fpr, tpr, colour = path, linetype = auc)) +\n  geom_line(size = 0.5) +\n  \n  geom_abline(color = \"green\", linetype = \"dotted\") +\n  \n  theme_new +\n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"ROC Plot\",\n    subtitle = \"Performance of 3 Top Performing Models\"\n  )\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n5 Precision vs Recall Plot\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 16 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.59 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nmodel_metrics_tbl %>%\n  mutate(\n    path = str_split(path, pattern = \"/\", simplify = T)[,2] %>% as_factor(),\n    auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(recall, precision, color = path, linetype = auc)) +\n  geom_line() +\n  theme_new + \n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"Precision vs Recall Plot\",\n    subtitle = \"Performance of 3 Top Performing Models\"\n  )\n\n\n\n\n\n\n\n\n6 Gain Plot\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 16 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.59 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\ngain_lift_tbl <- performance_h2o %>%\n  h2o.gainsLift() %>%\n  as.tibble()\n\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\ngain_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"lift\")) %>%\n  mutate(baseline = cumulative_data_fraction) %>%\n  rename(gain     = cumulative_capture_rate) %>%\n  \n  pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n\ngain_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.25) +\n  labs(\n    title = \"Gain Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Gain\"\n  ) +\n  theme_new\n\n\n\n\n\n\n\n\n7 Lift Plot\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 16 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.59 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nlift_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"capture\")) %>%\n  mutate(baseline = 1) %>%\n  rename(lift = cumulative_lift) %>%\n  pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n\nlift_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Lift Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Lift\"\n  ) +\n  theme_new\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n8 Dashboard with cowplot\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 16 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.59 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nlibrary(cowplot)\nlibrary(glue)\n\nh2o_leaderboard <- automl_models_h2o@leaderboard\nnewdata <- test_data\norder_by <- \"auc\"\nmax_models <- 3\nsize <- 1\n\nplot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c(\"auc\", \"logloss\"),\n                                 max_models = 3, size = 1) {\n    \n    # Inputs\n    \n    leaderboard_tbl <- h2o_leaderboard %>%\n        as_tibble() %>%\n        slice(1:max_models)\n    \n    newdata_tbl <- newdata %>%\n        as_tibble()\n\n    order_by      <- tolower(order_by[[1]]) \n\n    order_by_expr <- rlang::sym(order_by)\n\n    h2o.no_progress()\n    \n    # 1. Model metrics\n    \n    get_model_performance_metrics <- function(model_id, test_data) {\n        \n        model_h2o <- h2o.getModel(model_id)\n        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_data))\n        \n        perf_h2o %>%\n            h2o.metric() %>%\n            as.tibble() %>%\n            select(threshold, tpr, fpr, precision, recall)\n        \n    }\n    \n    model_metrics_tbl <- leaderboard_tbl %>%\n        mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%\n        unnest(cols = metrics) %>%\n        mutate(\n          model_id = as_factor(model_id) %>% \n                  \n                      fct_reorder(!! order_by_expr, \n                                  .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n          auc      = auc %>% \n                      round(3) %>% \n                      as.character() %>% \n                      as_factor() %>% \n                      fct_reorder(as.numeric(model_id)),\n          logloss  = logloss %>% \n                      round(4) %>% \n                      as.character() %>% \n                      as_factor() %>% \n                      fct_reorder(as.numeric(model_id))\n        )\n    \n    \n    # 1A. ROC Plot\n    \n    p1 <- model_metrics_tbl %>%\n        ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size) +\n        theme_new +\n        labs(title = \"ROC\", x = \"FPR\", y = \"TPR\") +\n        theme(legend.direction = \"vertical\") \n        \n    \n    # 1B. Precision vs Recall\n    \n    p2 <- model_metrics_tbl %>%\n        ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size) +\n        theme_new +\n        labs(title = \"Precision Vs Recall\", x = \"Recall\", y = \"Precision\") +\n        theme(legend.position = \"none\") \n    \n    \n    # 2. Gain / Lift\n    \n    get_gain_lift <- function(model_id, test_data) {\n        \n        model_h2o <- h2o.getModel(model_id)\n        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_data)) \n        \n        perf_h2o %>%\n            h2o.gainsLift() %>%\n            as.tibble() %>%\n            select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)\n        \n    }\n    \n    gain_lift_tbl <- leaderboard_tbl %>%\n        mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%\n        unnest(cols = metrics) %>%\n        mutate(\n            model_id = as_factor(model_id) %>% \n                fct_reorder(!! order_by_expr, \n                            .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n            auc  = auc %>% \n                round(3) %>% \n                as.character() %>% \n                as_factor() %>% \n                fct_reorder(as.numeric(model_id)),\n            logloss = logloss %>% \n                round(4) %>% \n                as.character() %>% \n                as_factor() %>% \n                fct_reorder(as.numeric(model_id))\n        ) %>%\n        rename(\n            gain = cumulative_capture_rate,\n            lift = cumulative_lift\n        ) \n    \n    # 2A. Gain Plot\n    \n    p3 <- gain_lift_tbl %>%\n        ggplot(aes(cumulative_data_fraction, gain, \n                          color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size,) +\n        geom_segment(x = 0, y = 0, xend = 1, yend = 1, \n                     color = \"red\", size = size, linetype = \"dotted\") +\n        theme_new +\n        expand_limits(x = c(0, 1), y = c(0, 1)) +\n        labs(title = \"Gain\",\n             x = \"Cumulative Data Fraction\", y = \"Gain\") +\n        theme(legend.position = \"none\")\n    \n    # 2B. Lift Plot\n    \n    p4 <- gain_lift_tbl %>%\n        ggplot(aes(cumulative_data_fraction, lift, \n                          color = model_id, linetype = !! order_by_expr)) +\n        geom_line(size = size) +\n        geom_segment(x = 0, y = 1, xend = 1, yend = 1, \n                     color = \"red\", size = size, linetype = \"dotted\") +\n        theme_new +\n        expand_limits(x = c(0, 1), y = c(0, 1)) +\n        labs(title = \"Lift\",\n             x = \"Cumulative Data Fraction\", y = \"Lift\") +\n        theme(legend.position = \"none\") \n    \n    \n    # Combine using cowplot\n    \n    # cowplot::get_legend extracts a legend from a ggplot object\n    p_legend <- get_legend(p1)\n    # Remove legend from p1\n    p1 <- p1 + theme(legend.position = \"none\")\n    \n    # cowplot::plt_grid() combines multiple ggplots into a single cowplot object\n    p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n    \n    # cowplot::ggdraw() sets up a drawing layer\n    p_title <- ggdraw() + \n    \n        # cowplot::draw_label() draws text on a ggdraw layer / ggplot object\n        draw_label(\"H2O Model Metrics\", size = 18, fontface = \"bold\", \n                   color = \"#2C3E50\")\n    \n    p_subtitle <- ggdraw() + \n        draw_label(glue(\"Ordered by {toupper(order_by)}\"), size = 10,  \n                   color = \"#2C3E50\")\n    \n    # Combine everything\n    ret <- plot_grid(p_title, p_subtitle, p, p_legend, \n    \n                     # Adjust the relative spacing, so that the legends always fits\n                     ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))\n    \n    h2o.show_progress()\n    \n    return(ret)\n    \n}\n\nautoml_models_h2o@leaderboard %>%\n    plot_h2o_performance(newdata = test_data, order_by = \"logloss\", \n                         size = 0.5, max_models = 4)\n\n#> Warning: There was 1 warning in `mutate()`.\n#> ℹ In argument: `metrics = map(model_id, get_model_performance_metrics,\n#>   newdata_tbl)`.\n#> Caused by warning:\n#> ! `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead."
  },
  {
    "objectID": "content/01_journal/Challenge_6.html",
    "href": "content/01_journal/Challenge_6.html",
    "title": "Explaining Black-Box Models With LIME",
    "section": "",
    "text": "library(h2o)\nlibrary(recipes)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(lime)\nlibrary(rsample)\n\nemployee_attrition_tbl <- read_csv(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/FUNDAMENTAL_DOWNLOADS/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.txt\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndefinitions_raw_tbl    <- read_excel(\"/Users/felixadamaszek/Documents/GitHub/ss23-bdml-FelixAdams1827/ss23-bdml-FelixAdams1827/content/01_journal/FUNDAMENTAL_DOWNLOADS/data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`"
  },
  {
    "objectID": "content/01_journal/Challenge_6.html#lime-and-single-explanation",
    "href": "content/01_journal/Challenge_6.html#lime-and-single-explanation",
    "title": "Explaining Black-Box Models With LIME",
    "section": "\n6.1 LIME and Single Explanation",
    "text": "6.1 LIME and Single Explanation\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 19 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.48 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\n# Predictions\n\npredictions_tbl <- automl_leader %>% \n  h2o.predict(newdata = as.h2o(test_tbl)) %>%\n  as.tibble() %>%\n  bind_cols(\n    test_tbl %>%\n      select(Attrition, EmployeeNumber)\n  )\n\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_tbl %>%\n  slice(4)\n\n\n\n  \n\n\n# Single Explanation\n\nexplainer <- train_tbl %>%\n  select(-Attrition) %>%\n  lime(\n    model           = automl_leader,\n    bin_continuous  = TRUE,\n    n_bins          = 4,\n    quantile_bins   = TRUE\n  )\n\nexplanation <- test_tbl %>%\n  slice(1) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    \n    # Pass our explainer object\n    explainer = explainer,\n    # Because it is a binary classification model: 1\n    n_labels   = 1,\n    # number of features to be returned\n    n_features = 8,\n    # number of localized linear models\n    n_permutations = 5000,\n    # Let's start with 1\n    kernel_width   = 1\n  )\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation %>%\n  as.tibble()\n\n\n\n  \n\n\n# Transform the explanation tibble into a data frame\n  \nexplanation_df <- as.data.frame(explanation)\n\n# Select only the columns \"feature\" and \"feature_weight\"\n\nplot_data <- explanation_df[, c(\"feature\", \"feature_weight\")]\n\n# Order the data by feature_weight to have a sorted barplot\n\nplot_data <- plot_data[order(plot_data$feature_weight), ]\n\n# Add new column \"Support\"\n\nplot_data$Support <- ifelse(plot_data$feature_weight > 0, \"Supports\", \"Contradicts\")\n\n# EmployeeNumber of the case for which the explanation was created\n\nexplained_employee <- test_tbl$EmployeeNumber[1]\n\n# Select the feature values for the explained case\n\nexplained_case_features <- test_tbl[1, ]\n\n# Convert the data to a tibble and data frame\n\nexplained_case_features %>%\n  as_tibble() \n\n\n\n  \n\n\nexplained_case_features_df <- as.data.frame(explained_case_features)\n\n# Loop through each feature in the explanation dataframe\n\nfor(i in 1:nrow(explanation_df)){\n  # Get the feature name\n  feature_name <- explanation_df$feature[i]\n  \n  # Find the corresponding value in `explained_case_features_df`\n  \n  feature_value <- explained_case_features_df[[feature_name]]\n  \n  # Modify the feature name to include the value\n  \n  explanation_df$feature[i] <- paste(feature_name, \"=\", feature_value)\n}\n\n# Plotting\n\nplot_data <- explanation_df[, c(\"feature\", \"feature_weight\")]\nplot_data <- plot_data[order(plot_data$feature_weight), ]\nplot_data$Support <- ifelse(plot_data$feature_weight > 0, \"Supports\", \"Contradicts\")\nggplot(data = plot_data, aes(x = reorder(feature, feature_weight), y = feature_weight, fill = Support)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = c(\"Supports\" = \"red\", \"Contradicts\" = \"green\")) +\n  labs(x = \"Features\", y = \"Weight\", title = \"Feature Importance\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/01_journal/Challenge_6.html#lime-and-multiple-explanation",
    "href": "content/01_journal/Challenge_6.html#lime-and-multiple-explanation",
    "title": "Explaining Black-Box Models With LIME",
    "section": "\n6.2 LIME and Multiple Explanation",
    "text": "6.2 LIME and Multiple Explanation\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 days 19 hours \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 11 days \n#>     H2O cluster name:           H2O_started_from_R_felixadamaszek_adl455 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.59 GB \n#>     H2O cluster total cores:    4 \n#>     H2O cluster allowed cores:  4 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15)\n\nexplanation <- test_tbl %>%\n  slice(1:20) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    explainer = explainer,\n    n_labels   = 1,\n    n_features = 8,\n    n_permutations = 5000,\n    kernel_width   = 0.5\n  )\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation %>%\n  as.tibble() %>%\n  print()\n\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n#> # A tibble: 160 × 13\n#>    model_type   case  label label_prob model_r2 model_intercept model_prediction\n#>    <chr>        <chr> <chr>      <dbl>    <dbl>           <dbl>            <dbl>\n#>  1 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  2 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  3 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  4 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  5 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  6 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  7 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  8 classificat… 1     No         0.642    0.295           0.860            0.561\n#>  9 classificat… 2     No         0.782    0.393           0.695            0.750\n#> 10 classificat… 2     No         0.782    0.393           0.695            0.750\n#> # ℹ 150 more rows\n#> # ℹ 6 more variables: feature <chr>, feature_value <dbl>, feature_weight <dbl>,\n#> #   feature_desc <chr>, data <list>, prediction <list>\n\n# Extract case number from 'case' column and convert to numeric\n\nexplanation$case <- as.numeric(str_extract(explanation$case, \"\\\\d+\"))\n\n# Create a dummy data frame with all possible combinations of cases and labels\n\ndummy_data <- expand.grid(case = unique(explanation$case), label = unique(explanation$label), feature = unique(explanation$feature))\ndummy_data$feature_weight <- NA\n\n# Identify columns present in 'explanation' but not in 'dummy_data'\n\nmissing_cols <- setdiff(names(explanation), names(dummy_data))\n\n# Add these columns to 'dummy_data' with NA values\nfor (col in missing_cols) {\n  dummy_data[[col]] <- NA\n}\n\nplot_data <- rbind(dummy_data, explanation)\n\n# Plotting\n\nggplot(data = plot_data, aes(x = as.factor(case), y = feature, fill = feature_weight)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"red\", high = \"white\", mid = \"yellow\", \n                       midpoint = 0, limit = c(-1,1), space = \"Lab\", \n                       name=\"Feature\\nWeight\", na.value = \"black\") +\n  facet_wrap(~ label, scales = \"free\", ncol = 2) +\n  labs(x = \"Cases\", y = \"Features\", fill = \"Feature Weight\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))"
  }
]